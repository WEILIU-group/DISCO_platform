"""
ML Platform (Strict SISSO Logic + Line Ending Fix)
- [SSH] å›æ»šåˆ° SISSO çš„ç›¸å¯¹è·¯å¾„é€»è¾‘ (ä½¿ç”¨ ~/path)ï¼Œç¡®ä¿ä¸ä½ çš„ç¯å¢ƒå…¼å®¹
- [Fix] å¼ºåˆ¶ slurm.sh ä½¿ç”¨ Linux æ¢è¡Œç¬¦ (\n)ï¼Œé˜²æ­¢ Windows ä¸Šä¼ å¯¼è‡´è„šæœ¬æ— æ³•æ‰§è¡Œ
- [UI] ä¿®å¤å¼¹çª—é®æŒ¡é—®é¢˜ (zIndex)
"""

import base64
import os
import io
import sys
import time
import json
import joblib
import requests
from pathlib import Path
import pandas as pd
import numpy as np
import dash
import dash_bootstrap_components as dbc
from dash import html, dcc, Input, Output, State, ctx, no_update, ALL, MATCH
from pymatgen.core import Structure, Lattice
from pymatgen.io.ase import AseAtomsAdaptor
import ase.io
import plotly.graph_objects as go
import zipfile
import traceback
from sklearn.model_selection import train_test_split

# Ensure local package import works when run as script
CURRENT_DIR = Path(__file__).resolve().parent
ROOT_DIR = CURRENT_DIR.parent
if str(ROOT_DIR) not in sys.path:
    sys.path.insert(0, str(ROOT_DIR))

from machine_learning.config import MLConfigManager
from machine_learning.data_loader import MLTrainDataBuilder
from services.common.templates import load_slurm_template
from services.config.loader import load_config, get_remote_server, get_queue_defaults

# Crystal Toolkit
import crystal_toolkit.components as ctc
from crystal_toolkit.settings import SETTINGS

# ----------------------------------------------------------------------------
# 0. é…ç½®ä¸åŸºç¡€æ•°æ®
# ----------------------------------------------------------------------------
ROOT_DIR = Path(__file__).resolve().parent.parent
ML_TEMPLATE_DIR = ROOT_DIR / "services" / "machine_learning"
GLOBAL_CONFIG = load_config()
REMOTE_CONFIG = get_remote_server(GLOBAL_CONFIG)
QUEUE_CONFIG = get_queue_defaults(GLOBAL_CONFIG)
BACKEND_BASE_URL = os.getenv("BACKEND_BASE_URL")


def build_server_options(cfg: dict):
    options = []
    mapping = {}

    clusters = (cfg.get("clusters") or {}).items()
    for name, cluster in clusters:
        remote = cluster.get("remote_server") or {}
        if not remote:
            continue
        mapping[name] = {
            "hostname": remote.get("hostname", ""),
            "username": remote.get("username", ""),
            "password": remote.get("password", ""),
            "port": remote.get("port", 22),
        }
        host = mapping[name]["hostname"]
        port = mapping[name]["port"]
        options.append({"label": f"{name} ({host}:{port})", "value": name})

    base = cfg.get("remote_server") or {}
    if base:
        mapping["_default"] = {
            "hostname": base.get("hostname", ""),
            "username": base.get("username", ""),
            "password": base.get("password", ""),
            "port": base.get("port", 22),
        }
        host = mapping["_default"]["hostname"]
        port = mapping["_default"]["port"]
        options.insert(0, {"label": f"é»˜è®¤ ({host}:{port})", "value": "_default"})

    options.append({"label": "è‡ªå®šä¹‰æœåŠ¡å™¨", "value": "custom"})
    return options, mapping


SERVER_OPTIONS, SERVER_MAP = build_server_options(GLOBAL_CONFIG)
DEFAULT_SERVER_KEY = SERVER_OPTIONS[0]["value"] if SERVER_OPTIONS else "custom"


def resolve_elements_csv():
    candidates = []
    cfg_name = GLOBAL_CONFIG.get("local_paths", {}).get("elements_csv", "elements_properties_all.csv")
    candidates.append(Path(cfg_name))
    candidates.append(ROOT_DIR / cfg_name)
    candidates.append(ROOT_DIR / "discriptor" / cfg_name)
    candidates.append(Path(__file__).resolve().parent / cfg_name)
    for p in candidates:
        if p.exists():
            return p
    print("Warning: elements_properties CSV not found; atom features will be empty.")
    return None


ELEMENTS_CSV_PATH = resolve_elements_csv()
ELEMENTS_DF = pd.read_csv(ELEMENTS_CSV_PATH) if ELEMENTS_CSV_PATH else None

# å¯ç”¨ç‰¹å¾åˆ—ï¼ˆæ•°å€¼å‹ï¼‰
AVAILABLE_FEATURES = []
if ELEMENTS_DF is not None:
    AVAILABLE_FEATURES = [c for c in ELEMENTS_DF.select_dtypes(include=["number"]).columns if c not in {"atomic_number"}]
    # æˆªå–å¸¸ç”¨å‰è‹¥å¹²ä¸ªï¼Œé¿å…ä¸‹æ‹‰è¿‡é•¿
    if len(AVAILABLE_FEATURES) > 20:
        AVAILABLE_FEATURES = AVAILABLE_FEATURES[:20]
if not AVAILABLE_FEATURES:
    AVAILABLE_FEATURES = ["atomic_number", "atomic_radius", "density"]

CONFIG_MANAGER = MLConfigManager()
TRADITIONAL_MODEL_OPTIONS = list(CONFIG_MANAGER.config.get("models", {}).get("traditional", {}).keys()) or ["xgb", "rf"]
GNN_MODEL_OPTIONS = list(CONFIG_MANAGER.config.get("models", {}).get("gnn", {}).keys()) or ["schnet", "dimenet_pp"]


def submit_job_via_backend(module: str, command: str, files: list[dict], remote_subdir: str = "ml"):
    if not BACKEND_BASE_URL:
        return None, "BACKEND_BASE_URL not set"
    try:
        resp = requests.post(
            f"{BACKEND_BASE_URL}/jobs/",
            json={
                "module": module,
                "command": command,
                "files": files,
                "remote_subdir": remote_subdir,
            },
            timeout=30,
        )
        if resp.status_code >= 300:
            return None, f"Backend rejected: {resp.text}"
        data = resp.json()
        return data, None
    except Exception as exc:
        return None, str(exc)


def refresh_job_via_backend(pk: int):
    if not BACKEND_BASE_URL:
        return None, "BACKEND_BASE_URL not set"
    try:
        resp = requests.post(f"{BACKEND_BASE_URL}/jobs/{pk}/refresh/", timeout=20)
        if resp.status_code >= 300:
            return None, f"Backend refresh failed: {resp.text}"
        return resp.json(), None
    except Exception as exc:
        return None, str(exc)

class MLFeatureBuilder:
    @staticmethod
    def extract_features(structures, indices_str, selected_features):
        if ELEMENTS_DF is None:
            return None, "ç¼ºå°‘å…ƒç´ ç‰¹å¾è¡¨ (elements_properties_all.csv)"

        data_rows = []
        try:
            indices = [int(i)-1 for i in indices_str.strip().split()]
        except: return None, "ç´¢å¼•æ ¼å¼é”™è¯¯"
        
        if not selected_features: return None, "æœªé€‰æ‹©ç‰¹å¾"

        for struct_info in structures:
            fname = struct_info['filename']
            s_obj = parse_structure_content(struct_info['content'])
            if not s_obj: continue

            row = {'filename': os.path.splitext(fname)[0]}
            valid = True
            for i, atom_idx in enumerate(indices):
                if atom_idx >= len(s_obj):
                    valid = False; break
                sym = s_obj[atom_idx].specie.symbol
                props_row = ELEMENTS_DF[ELEMENTS_DF['symbol'].str.lower() == sym.lower()]
                for feat in selected_features:
                    if not props_row.empty and feat in props_row.columns:
                        val = props_row.iloc[0][feat]
                        row[f"Atom{i+1}_{feat}"] = 0.0 if pd.isna(val) else float(val)
                    else:
                        row[f"Atom{i+1}_{feat}"] = 0.0
            if valid: data_rows.append(row)
        
        if not data_rows: return None, "æå–å¤±è´¥"
        return pd.DataFrame(data_rows), f"æå–æˆåŠŸ: {len(data_rows)} è¡Œ"

# ----------------------------------------------------------------------------
# 1. SSH ç®¡ç† (å¤ç”¨é€šç”¨ SSHManager + æ¢è¡Œç¬¦ä¿®å¤)
# ----------------------------------------------------------------------------
from services.remote_server.ssh_manager import SSHManager as BaseSSHManager


class RealSSHManager:
    """Thin wrapper to keep legacy interface while delegating to shared SSHManager."""

    def __init__(self, hostname, username, password, port=22):
        self._base = BaseSSHManager(hostname=hostname, port=int(port), username=username, password=password)

    def connect(self):
        ok, msg = self._base.connect()
        if ok:
            self._base.open_sftp()
        return ok, msg

    def mkdir_remote(self, dir_name):
        return self._base.mkdir_remote(dir_name)

    def write_remote_file(self, dir_name, filename, content):
        remote_path = f"{dir_name}/{filename}"
        clean_content = str(content).replace('\r\n', '\n')
        return self._base.write_remote_file(remote_path, clean_content)

    def write_remote_binary(self, dir_name, filename, data: bytes):
        """Write binary payloads (e.g., joblib) to remote using SFTP."""
        try:
            if not getattr(self._base, "sftp", None):
                return False, "SFTP æœªè¿æ¥"
            remote_path = f"{dir_name}/{filename}"
            with self._base.sftp.file(remote_path, "wb") as f:
                f.write(data)
            return True, f"å†™å…¥æ–‡ä»¶æˆåŠŸ: {remote_path}"
        except Exception as e:
            return False, f"å†™å…¥æ–‡ä»¶å¤±è´¥: {e}"

    def exec_command(self, cmd):
        ret, out, err = self._base.exec_command(cmd)
        return out, err

    def submit_job_slurm(self, dir_name):
        return self._base.submit_job_slurm(dir_name)

    def check_job_status(self, job_id):
        exists, _ = self._base.query_slurm_status(job_id)
        return "RUNNING" if exists else "COMPLETED"

    def download_file(self, dir_name, filename):
        success, content = self._base.read_remote_file(f"{dir_name}/{filename}")
        return content if success else None

    def close(self):
        self._base.close()

# ----------------------------------------------------------------------------
# 2. é…ç½®ä¸è§£æ
# ----------------------------------------------------------------------------
DEFAULT_CONFIG = SERVER_MAP.get(DEFAULT_SERVER_KEY) or {
    "hostname": REMOTE_CONFIG.get("hostname", ""),
    "username": REMOTE_CONFIG.get("username", ""),
    "password": REMOTE_CONFIG.get("password", ""),
    "port": REMOTE_CONFIG.get("port", 22),
}


def load_template_file(name: str) -> str:
    candidates = [
        ML_TEMPLATE_DIR / name,
        Path(__file__).resolve().parent / "templates" / name,
    ]
    for path in candidates:
        if path.exists():
            with open(path, "r", encoding="utf-8") as f:
                return f.read()
    raise FileNotFoundError(f"Template {name} not found in {candidates}")


TRAIN_SCRIPT_TEMPLATE = load_template_file("train_script.py")
try:
    SLURM_TEMPLATE = load_slurm_template(path=ML_TEMPLATE_DIR / "slurm.sh")
except FileNotFoundError:
    SLURM_TEMPLATE = load_template_file("slurm.sh")

def parse_structure_content(content):
    if not content: return None
    try:
        if "data_" in content[:500] or "_cell_" in content[:1000]:
            return Structure.from_str(content, fmt="cif")
        return Structure.from_str(content, fmt="poscar")
    except:
        try:
            atoms = ase.io.read(io.StringIO(content))
            return AseAtomsAdaptor.get_structure(atoms)
        except: return None


def build_corr_warning(full_df: pd.DataFrame, target_col: str):
    """For traditional models: compute Pearson corr on numeric feature cols (excluding filename/target),
    return plotly heatmap figure and warning text for |corr| > 0.8."""
    feature_df = full_df.drop(columns=[c for c in ["filename", target_col] if c in full_df.columns])
    feature_df = feature_df.select_dtypes(include=[np.number])
    if feature_df.shape[1] < 2:
        return None, "ç‰¹å¾åˆ—ä¸è¶³ï¼Œæ— æ³•è®¡ç®—ç›¸å…³æ€§ã€‚"

    corr = feature_df.corr(method="pearson").fillna(0)
    fig = go.Figure(data=go.Heatmap(z=corr.values, x=corr.columns, y=corr.columns, colorscale="RdBu", zmin=-1, zmax=1, colorbar=dict(title="Pearson")))
    fig.update_layout(margin=dict(l=40, r=40, t=40, b=40), height=480, template="plotly_white")

    high_pairs = []
    cols = corr.columns
    for i in range(len(cols)):
        for j in range(i):
            val = corr.iloc[i, j]
            if abs(val) >= 0.8:
                high_pairs.append((cols[i], cols[j], float(val)))

    if high_pairs:
        warn_text = "ä»¥ä¸‹ç‰¹å¾å¯¹ç›¸å…³æ€§ |r| â‰¥ 0.8ï¼Œè¯·ç•™æ„å…±çº¿æ€§: " + "; ".join([f"{a} vs {b}: {v:.2f}" for a, b, v in high_pairs])
    else:
        warn_text = "æœªå‘ç° |r| â‰¥ 0.8 çš„ç‰¹å¾å¯¹ã€‚"

    warning_block = dbc.Alert(warn_text, color="warning" if high_pairs else "success", className="mt-2 mb-1")
    heatmap_block = dcc.Graph(figure=fig, style={"height": "520px"})
    return heatmap_block, warning_block

# ----------------------------------------------------------------------------
# 3. UI
# ----------------------------------------------------------------------------
app = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP, "https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.5/font/bootstrap-icons.css"], suppress_callback_exceptions=True)
server = app.server

new_batch_uploader = dcc.Upload(
    id="new-batch-uploader",
    children=html.Div([
        html.I(className="bi bi-cloud-upload", style={"fontSize": "2rem"}),
        html.Div("æ‹–å…¥ç»“æ„æ–‡ä»¶"),
        html.Div("ç”Ÿæˆæ–°æ‰¹æ¬¡", className="text-muted small")
    ]),
    className="upload-container",
    multiple=True
)

server_card = dbc.Card([
    dbc.CardHeader("0. æœåŠ¡å™¨é€‰æ‹©"),
    dbc.CardBody([
        dbc.Label("é€‰æ‹©é¢„è®¾ / è‡ªå®šä¹‰", className="small fw-bold"),
        dcc.Dropdown(
            id="server-select",
            options=SERVER_OPTIONS,
            value=DEFAULT_SERVER_KEY,
            clearable=False,
            className="mb-2"
        ),
        dbc.Row([
            dbc.Col(dbc.Input(id="inp-host", placeholder="hostname", value=DEFAULT_CONFIG.get("hostname", "")), width=6),
            dbc.Col(dbc.Input(id="inp-port", placeholder="port", type="number", value=DEFAULT_CONFIG.get("port", 22)), width=6),
        ], className="g-2 mb-2"),
        dbc.Row([
            dbc.Col(dbc.Input(id="inp-user", placeholder="username", value=DEFAULT_CONFIG.get("username", "")), width=6),
            dbc.Col(dbc.Input(id="inp-pass", placeholder="password", type="password", value=DEFAULT_CONFIG.get("password", "")), width=6),
        ], className="g-2 mb-2"),
        dbc.Button("åº”ç”¨è‡ªå®šä¹‰æœåŠ¡å™¨", id="btn-apply-server", color="secondary", size="sm", className="w-100"),
        html.Div(id="server-hint", className="text-muted small mt-2")
    ])
], className="mb-3")

settings_card = dbc.Card([
    dbc.CardHeader("2. é…ç½®"),
    dbc.CardBody([
        dbc.Label("ç‰¹å¾ (Atom Features)", className="small fw-bold"),
        dcc.Dropdown(
            id="inp-features",
            options=[{'label': f, 'value': f} for f in AVAILABLE_FEATURES],
            value=AVAILABLE_FEATURES[:3],
            multi=True,
            className="mb-2"
        ),
        dbc.Label("æ¨¡å‹ (Model)", className="small fw-bold"),
        dcc.Dropdown(
            id="inp-model",
            options=[{'label': f"Trad | {m.upper()}", 'value': m} for m in TRADITIONAL_MODEL_OPTIONS] +
                    [{'label': f"GNN | {m.upper()}", 'value': m} for m in GNN_MODEL_OPTIONS],
            value=(TRADITIONAL_MODEL_OPTIONS[0] if TRADITIONAL_MODEL_OPTIONS else (GNN_MODEL_OPTIONS[0] if GNN_MODEL_OPTIONS else 'xgb')),
            clearable=False
        ),
        html.Hr(),
        dbc.Button("åˆå¹¶æ•°æ®å¹¶ç”Ÿæˆä»»åŠ¡", id="btn-generate", color="primary", className="w-100"),
        html.Div(id="log-gen", style={"height": "60px", "overflowY": "scroll", "backgroundColor": "#111", "color": "#0f0", "fontSize": "11px", "padding": "5px", "marginTop": "5px", "whiteSpace": "pre-wrap"})
    ])
])

modal = dbc.Modal([
    dbc.ModalHeader("é¢„è§ˆä¸æäº¤"),
    dbc.ModalBody(dbc.Tabs([
        dbc.Tab(label="train_script.py", label_class_name="fw-semibold text-dark", children=[
            html.Label("train_script.py", className="fw-bold mb-1"),
            dcc.Textarea(
                id="editor-script",
                style={"width": "100%", "height": "380px", "fontFamily": "monospace", "backgroundColor": "#f8f9fa", "color": "#111", "border": "1px solid #dee2e6"}
            )
        ]),
        dbc.Tab(label="slurm.sh", label_class_name="fw-semibold text-dark", children=[
            html.Label("slurm.sh", className="fw-bold mb-1"),
            dcc.Textarea(
                id="editor-slurm",
                style={"width": "100%", "height": "380px", "fontFamily": "monospace", "backgroundColor": "#f8f9fa", "color": "#111", "border": "1px solid #dee2e6"}
            )
        ]),
        dbc.Tab(label="config.json", label_class_name="fw-semibold text-dark", children=[
            html.Label("config.json", className="fw-bold mb-1"),
            dcc.Textarea(
                id="editor-config",
                style={"width": "100%", "height": "380px", "fontFamily": "monospace", "whiteSpace": "pre", "backgroundColor": "#f8f9fa", "color": "#111", "border": "1px solid #dee2e6"}
            )
        ]),
        dbc.Tab(label="train_data.csv", label_class_name="fw-semibold text-dark", children=[
            html.Label("train_data.csv", className="fw-bold mb-1"),
            dcc.Textarea(
                id="editor-data",
                style={"width": "100%", "height": "380px", "fontFamily": "monospace", "whiteSpace": "pre", "backgroundColor": "#f8f9fa", "color": "#111", "border": "1px solid #dee2e6"}
            )
        ]),
    ])),
    dbc.ModalFooter([
        html.Div(id="log-sub", className="me-auto small text-muted"),
        dbc.Button("æ‹‰å–çŠ¶æ€", id="btn-pull-status", outline=True, color="warning", className="me-2"),
        dbc.Button("å–æ¶ˆ", id="btn-close-modal", className="me-2"),
        dbc.Button("æäº¤", id="btn-submit-modal", color="primary")
    ])
], id="modal-file-editor", size="xl", backdrop="static", style={"zIndex": 100000})

left_panel = [
    server_card,
    dbc.Card([dbc.CardHeader("1. æ–°å»º (New Batch)", className="bg-primary text-white py-2"), dbc.CardBody([new_batch_uploader], className="p-2")], className="mb-3"),
    html.Div(settings_card, className="mb-3")
]

right_panel = [
    dbc.Card([
        dbc.CardHeader(["å·¥ä½œåŒº", dbc.Button("æ¸…ç©º", id="btn-reset-all", color="link", size="sm", className="float-end text-danger text-decoration-none py-0")]), 
        dbc.CardBody([html.Div(id="batches-container", className="row g-2"), html.Div("è¯·æ‹–å…¥ç»“æ„...", id="empty-placeholder", className="text-center text-muted py-5")], className="p-2")
    ], className="mb-3"),
    dbc.Card([dbc.CardHeader("4. ç»“æœ"), dbc.CardBody(html.Div(id='result-display'))])
]

ctc.register_crystal_toolkit(app=app, layout=dbc.Container([
    modal,
    dcc.Store(id='store-server-config', data=DEFAULT_CONFIG, storage_type="session"),
    dcc.Store(id='store-batches-data', data=[]),
    dcc.Store(id='store-job-info', data={}),
    dcc.Interval(id='interval-job-monitor', interval=5000),
    dbc.NavbarSimple(
        brand="ğŸ¤– ML Feature Building",
        color="white", className="mb-3 shadow-sm",
        children=[dbc.NavItem(dbc.NavLink("Reset", href="/", external_link=True))]
    ), 
    dbc.Row([dbc.Col(left_panel, width=3), dbc.Col(right_panel, width=9)])
], fluid=True, style={"minHeight": "100vh"}))


if __name__ == "__main__":
    # Dash å…¥å£ï¼Œå›ºå®šç«¯å£ 8052ï¼Œä¸å…¶ä»–æ¨¡å— (æè¿°ç¬¦ 8050 / é«˜é€šé‡ 8051) åŒºåˆ†
    app.run(debug=True, port=8052)

# ----------------------------------------------------------------------------
# 4. å›è°ƒå‡½æ•°
# ----------------------------------------------------------------------------

# 0. æœåŠ¡å™¨é€‰æ‹©
@app.callback(
    Output("store-server-config", "data"),
    Output("inp-host", "value"),
    Output("inp-port", "value"),
    Output("inp-user", "value"),
    Output("inp-pass", "value"),
    Output("server-select", "value"),
    Output("server-hint", "children"),
    Input("server-select", "value"),
    Input("btn-apply-server", "n_clicks"),
    State("inp-host", "value"),
    State("inp-port", "value"),
    State("inp-user", "value"),
    State("inp-pass", "value"),
)
def sync_server_config(select_value, n_apply, host, port, user, pwd):
    trig = ctx.triggered_id
    port_val = int(port) if str(port).isdigit() else 22

    def pack(cfg: dict, value: str, hint: str):
        cfg = cfg or {}
        return (
            cfg,
            cfg.get("hostname", ""),
            cfg.get("port", 22),
            cfg.get("username", ""),
            cfg.get("password", ""),
            value,
            hint,
        )

    if trig == "server-select":
        if select_value in SERVER_MAP:
            cfg = SERVER_MAP.get(select_value, {})
            return pack(cfg, select_value, f"å·²é€‰æ‹©é¢„è®¾: {select_value}")
        cfg = {
            "hostname": host or "",
            "port": port_val,
            "username": user or "",
            "password": pwd or "",
        }
        return pack(cfg, "custom", "è‡ªå®šä¹‰æœªä¿å­˜ï¼Œç‚¹å‡»ä¸‹æ–¹åº”ç”¨")

    if trig == "btn-apply-server":
        cfg = {
            "hostname": host or "",
            "port": port_val,
            "username": user or "",
            "password": pwd or "",
        }
        return pack(cfg, "custom", "å·²åº”ç”¨è‡ªå®šä¹‰æœåŠ¡å™¨")

    fallback = SERVER_MAP.get(select_value) or DEFAULT_CONFIG
    return pack(fallback, select_value or DEFAULT_SERVER_KEY, "å·²åŠ è½½é»˜è®¤æœåŠ¡å™¨")


# 1. æ‰¹æ¬¡åˆ›å»º
@app.callback(
    Output("store-batches-data", "data"), Output("batches-container", "children"), Output("empty-placeholder", "style"), Output("new-batch-uploader", "contents"),
    Input("new-batch-uploader", "contents"), Input("btn-reset-all", "n_clicks"),
    State("new-batch-uploader", "filename"), State("store-batches-data", "data")
)
def manage_batches_safe(contents, n_reset, filenames, current_data):
    if ctx.triggered_id == "btn-reset-all": return [], [], {"display": "block"}, None
    try:
        if current_data is None: current_data = []
        updated_data = current_data
        if contents:
            new_structs = []
            for c, f in zip(contents, filenames):
                try:
                    decoded = base64.b64decode(c.split(",")[1])
                    if f.endswith('.zip'):
                        with zipfile.ZipFile(io.BytesIO(decoded)) as z:
                            for n in z.namelist():
                                if not n.endswith('/') and not n.startswith('__MACOSX'):
                                    new_structs.append({'filename': os.path.basename(n), 'content': z.read(n).decode('utf-8', errors='ignore')})
                    else:
                        new_structs.append({'filename': f, 'content': decoded.decode('utf-8', errors='ignore')})
                except Exception as e: print(f"File Error: {e}")
            if new_structs: updated_data.append({"id": len(current_data), "structures": new_structs})

        children = []
        for batch in updated_data:
            b_id = batch['id']
            structs = batch['structures']
            init_struct = None
            if structs: init_struct = parse_structure_content(structs[0]['content'])
            if init_struct is None: init_struct = Structure(Lattice.cubic(3.0), ["H"], [[0,0,0]])

            ctc_view = ctc.StructureMoleculeComponent(init_struct, id=f"viewer-batch-{b_id}", color_scheme="VESTA")
            
            card = dbc.Col(dbc.Card([
                dbc.CardHeader([
                    dbc.Row([
                        dbc.Col([html.Strong(f"Batch #{b_id+1}"), html.Span(f"{len(structs)}", className="badge bg-secondary ms-1")], width="auto"),
                        dbc.Col([dcc.Dropdown(id={'type': 'struct-sel', 'index': b_id}, options=[{'label': s['filename'], 'value': i} for i, s in enumerate(structs)], value=0, clearable=False, style={"fontSize": "12px"})], width=4),
                        dbc.Col([dbc.Input(id={'type': 'indices-inp', 'index': b_id}, placeholder="Index (e.g. 1 2)", size="sm")], width=3),
                        dbc.Col([dcc.Upload(id={'type': 'csv-up', 'index': b_id}, children=html.Div("CSV", id={'type': 'csv-lbl', 'index': b_id}, className="badge bg-secondary"), style={"cursor": "pointer"})], width=True)
                    ], className="g-1 align-items-center")
                ]),
                dbc.CardBody(html.Div(ctc_view.layout(), style={"height": "300px", "width": "100%", "position": "relative"}))
            ], className="shadow-sm border-0 mb-3"), width=12, lg=6)
            children.append(card)
        return updated_data, children, {"display": "none"} if children else {"display": "block"}, None
    except: return no_update, no_update, no_update, None

# 2. åŠ¨æ€è§†å›¾
@app.callback([Output(f"viewer-batch-{i}", "data") for i in range(20)], Input({'type': 'struct-sel', 'index': ALL}, 'value'), State("store-batches-data", "data"))
def update_view(vals, data):
    outs = [no_update] * 20
    if not ctx.triggered_id: return outs
    idx = ctx.triggered_id['index']
    if idx < len(data):
        try:
            s = parse_structure_content(data[idx]['structures'][vals[idx]]['content'])
            if s: outs[idx] = s
        except: pass
    return outs

# 3. CSV
@app.callback(Output({'type': 'csv-lbl', 'index': MATCH}, 'children'), Output({'type': 'csv-lbl', 'index': MATCH}, 'className'), Input({'type': 'csv-up', 'index': MATCH}, 'contents'))
def csv_lbl(c): return ("OK", "badge bg-success") if c else no_update

# 4. ç”Ÿæˆä¸æäº¤
@app.callback(
    [Output("log-gen", "children"), Output("editor-script", "value"), Output("editor-slurm", "value"), Output("editor-config", "value"), Output("editor-data", "value"), Output("modal-file-editor", "is_open"), 
     Output("store-job-info", "data"), Output("log-sub", "children"), Output("result-display", "children")],
    [Input("btn-generate", "n_clicks"), Input("btn-close-modal", "n_clicks"), Input("btn-submit-modal", "n_clicks"), Input("interval-job-monitor", "n_intervals"), Input("btn-pull-status", "n_clicks")],
    [State("store-batches-data", "data"), State({'type': 'csv-up', 'index': ALL}, 'contents'), State({'type': 'indices-inp', 'index': ALL}, 'value'),
     State("inp-features", "value"), State("inp-model", "value"), State("log-gen", "children"), 
    State("editor-script", "value"), State("editor-slurm", "value"), State("editor-config", "value"), State("editor-data", "value"), State("store-server-config", "data"), State("store-job-info", "data"), State("log-sub", "children")]
)
def task_manager(n_gen, n_cl, n_sub, n_int, n_pull, batches, csvs, idxs, feats, model, l_gen, scr, slu, cfg, dat, server_cfg, job, l_sub):
    trig = ctx.triggered_id
    active_server = server_cfg or DEFAULT_CONFIG
    
    if trig == "btn-generate":
        if not batches:
            return (l_gen or "")+"\næ— æ‰¹æ¬¡æ•°æ®", no_update, no_update, no_update, no_update, no_update, no_update, no_update, no_update

        is_gnn = model in GNN_MODEL_OPTIONS
        logs, dfs, graph_dataset = [], [], []
        data_builder = MLTrainDataBuilder(ELEMENTS_DF if ELEMENTS_DF is not None else pd.DataFrame())

        for i, b in enumerate(batches):
            idx_str = idxs[i] if i < len(idxs) else None
            csv_content = csvs[i] if i < len(csvs) else None

            if is_gnn:
                if not csv_content:
                    logs.append(f"Batch {i+1}: GNN éœ€è¦ CSV ç›®æ ‡åˆ—")
                    continue
                try:
                    c_df = pd.read_csv(io.StringIO(base64.b64decode(csv_content.split(',')[1]).decode('utf-8')))
                    if 'filename' not in c_df:
                        c_df.rename(columns={c_df.columns[0]: 'filename'}, inplace=True)
                    c_df['filename'] = c_df['filename'].astype(str).apply(lambda x: x.split('.')[0])
                    if len(c_df.columns) < 2:
                        logs.append(f"Batch {i+1}: CSV ç¼ºå°‘ç›®æ ‡åˆ—ï¼ˆè‡³å°‘éœ€è¦2åˆ—ï¼šid, targetï¼‰")
                        continue
                    target_col = c_df.columns[1]
                    targets_map = dict(zip(c_df['filename'], c_df[target_col]))
                    part = data_builder.build_graph_dataset(b['structures'], targets_map, parse_structure_func=parse_structure_content) if data_builder else []
                    if part:
                        graph_dataset.extend(part)
                        logs.append(f"Batch {i+1}: å›¾æ•°æ® {len(part)} æ¡ (ç›®æ ‡åˆ— {target_col})")
                    else:
                        logs.append(f"Batch {i+1}: å›¾æ•°æ®ç”Ÿæˆå¤±è´¥")
                except Exception as e:
                    logs.append(f"CSV/GNN Error: {e}")
                continue

            # ä¼ ç»Ÿæ¨¡å‹è·¯å¾„
            if not idx_str:
                logs.append(f"Batch {i+1}: æ— ç´¢å¼•")
                continue
            feat_df, msg = MLFeatureBuilder.extract_features(b['structures'], idx_str, feats)
            logs.append(f"Batch {i+1}: {msg}")
            if feat_df is not None:
                if csv_content:
                    try:
                        c_df = pd.read_csv(io.StringIO(base64.b64decode(csv_content.split(',')[1]).decode('utf-8')))
                        if 'filename' not in c_df: c_df.rename(columns={c_df.columns[0]: 'filename'}, inplace=True)
                        c_df['filename'] = c_df['filename'].astype(str).apply(lambda x: x.split('.')[0])

                        if len(c_df.columns) < 2:
                            logs.append(f"Batch {i+1}: CSV ç¼ºå°‘ç›®æ ‡åˆ—ï¼ˆè‡³å°‘éœ€è¦2åˆ—ï¼šid, target[, extra...]ï¼‰")
                            continue

                        target_col = c_df.columns[1]  # æŒ‰æè¿°ç¬¦é€»è¾‘ï¼Œç¬¬äºŒåˆ—ä¸ºç›®æ ‡
                        extra_cols_csv = list(c_df.columns[2:])

                        merge_df = pd.merge(feat_df, c_df, on='filename')

                        # é‡æ’: filename + ç»“æ„ç‰¹å¾ + CSVé¢å¤–ç‰¹å¾ + ç›®æ ‡åˆ—
                        feat_cols = list(feat_df.columns)
                        extra_cols_merged = [c for c in extra_cols_csv if c in merge_df.columns]
                        ordered_cols = feat_cols + extra_cols_merged + [target_col]
                        # å»é‡ä»¥é˜²ä¸‡ä¸€
                        ordered_cols = [c for i, c in enumerate(ordered_cols) if c not in ordered_cols[:i]]
                        merge_df = merge_df[ordered_cols]
                        feat_df = merge_df
                        logs.append(f"Batch {i+1}: ç›®æ ‡åˆ— '{target_col}' æ”¾åˆ°æœ€åï¼Œé¢å¤–ç‰¹å¾ {extra_cols_merged}")
                    except Exception as e:
                        logs.append(f"CSV Error: {e}")
                dfs.append(feat_df)
        
        if is_gnn:
            if not graph_dataset:
                return (l_gen or "")+"\nGNN æ•°æ®ç”Ÿæˆå¤±è´¥", no_update, no_update, no_update, no_update, no_update, no_update, no_update, no_update
            train_set, test_set = train_test_split(graph_dataset, test_size=0.2, random_state=42)
            payload = (train_set, None, test_set, None)
            buf = io.BytesIO()
            joblib.dump(payload, buf)
            pkl_b64 = base64.b64encode(buf.getvalue()).decode('utf-8')
            data_preview = f"GNN dataset: total {len(graph_dataset)}, train {len(train_set)}, test {len(test_set)}"
            corr_view = dbc.Alert("GNN æ¨¡å‹è·³è¿‡ç›¸å…³æ€§æ£€æŸ¥", color="info", className="mt-2 mb-0")
            model_params = CONFIG_MANAGER.get_model_params("gnn", model) or {}
            config_payload = json.dumps({"model_name": model, "params": model_params}, indent=4)
            job_payload = {"job_name": f"ml_job_{int(time.time())}", "dataset_type": "graph", "pkl_b64": pkl_b64}
            job_name = job_payload["job_name"]
        else:
            if not dfs:
                return (l_gen or "")+"\nå¤±è´¥", no_update, no_update, no_update, no_update, no_update, no_update, no_update, no_update

            full_df = pd.concat(dfs).fillna(0)
            data_preview = full_df.to_csv(index=False)
            # Pearson correlation check (traditional models only)
            target_col = full_df.columns[-1]
            heatmap_block, warning_block = build_corr_warning(full_df, target_col)
            corr_view = html.Div([
                html.H6("ç‰¹å¾ç›¸å…³æ€§ (Pearson)", className="fw-bold mt-2"),
                warning_block if warning_block else html.Div("æ— ç›¸å…³æ€§ç»“æœ"),
                heatmap_block if heatmap_block else html.Div()
            ], className="p-2 bg-light border rounded")
            model_params = CONFIG_MANAGER.get_model_params("traditional", model) or {}
            config_payload = json.dumps({"model_name": model, "params": model_params}, indent=4)
            job_payload = {"job_name": f"ml_job_{int(time.time())}", "dataset_type": "tabular"}
            job_name = job_payload["job_name"]

        fmt_kwargs = {
            "job_name": job_name,
            "nodes": QUEUE_CONFIG.get("nodes", 1),
            "ntasks": QUEUE_CONFIG.get("ntasks_per_node", 30),
            "time_limit": QUEUE_CONFIG.get("time_limit", "01:00:00"),
            "partition": QUEUE_CONFIG.get("partition", "vasp"),
            # Support shared template placeholders
            "command": "python train_script.py",
            "email_directive": "",
            # Support legacy local template placeholder
            "env_name": "ml_env",
        }
        slurm_content = SLURM_TEMPLATE.format(**fmt_kwargs)
        script_content = TRAIN_SCRIPT_TEMPLATE.replace("{model_name}", model)

        return (
            (l_gen or "")+"\n" + " | ".join(logs) if logs else (l_gen or ""),
            script_content,
            slurm_content,
            config_payload,
            data_preview if data_preview else "ç”Ÿæˆçš„ train_data ä¸ºç©º",
            True,
            job_payload,
            no_update,
            corr_view,
        )

    elif trig == "btn-close-modal":
            return no_update, no_update, no_update, no_update, no_update, False, no_update, no_update, no_update

    elif trig == "btn-submit-modal":
        dataset_type = (job or {}).get("dataset_type", "tabular")
        pkl_b64 = (job or {}).get("pkl_b64")

        cfg_content = cfg
        if not cfg_content:
            params = CONFIG_MANAGER.get_model_params("gnn", model) if dataset_type == "graph" else CONFIG_MANAGER.get_model_params("traditional", model) or {}
            cfg_content = json.dumps({"model_name": model, "params": params}, indent=4)

        if not scr or not slu or (not dat and dataset_type != "graph"):
            return no_update, no_update, no_update, no_update, no_update, no_update, no_update, "æ–‡ä»¶å†…å®¹ç¼ºå¤±ï¼Œè¯·é‡æ–°ç”Ÿæˆ", no_update
        if dataset_type == "graph" and not pkl_b64:
            return no_update, no_update, no_update, no_update, no_update, no_update, no_update, "ç¼ºå°‘å›¾æ•°æ®æ–‡ä»¶ï¼Œè¯·é‡æ–°ç”Ÿæˆ", no_update

        use_backend = bool(BACKEND_BASE_URL) and dataset_type != "graph"
        if use_backend:
            files = [
                {"name": "train_script.py", "content": scr},
                {"name": "slurm.sh", "content": slu},
                {"name": "config.json", "content": cfg_content},
                {"name": "train_data.csv", "content": dat},
            ]
            data, err = submit_job_via_backend(
                module="machine_learning",
                command="bash train_script.py",
                files=files,
                remote_subdir="ml",
            )
            if err or not data:
                return no_update, no_update, no_update, no_update, no_update, no_update, no_update, f"åç«¯æäº¤å¤±è´¥: {err}", no_update

            job_info = {
                "id": data.get("job_id"),
                "dir": data.get("remote_dir"),
                "status": "sub",
                "dataset_type": dataset_type,
                "backend_pk": data.get("id"),
                "server": active_server,
            }
            log_msg = f"å·²é€šè¿‡ Django åç«¯æäº¤: job={job_info['id']} dir={job_info['dir']}"
            return no_update, no_update, no_update, no_update, no_update, False, job_info, log_msg, no_update

        ssh = RealSSHManager(**active_server)
        ok, msg = ssh.connect()
        if not ok:
            return no_update, no_update, no_update, no_update, no_update, no_update, no_update, f"è¿æ¥å¤±è´¥: {msg}", no_update

        rd = f"ML_{int(time.time())}"
        ok_mk, msg_mk = ssh.mkdir_remote(rd)
        if not ok_mk:
            return no_update, no_update, no_update, no_update, no_update, no_update, no_update, f"{msg_mk}", no_update

        log_steps = [f"ç›®å½•: {rd}"]
        ok1, m1 = ssh.write_remote_file(rd, "train_script.py", scr)
        log_steps.append(m1)
        ok2, m2 = ssh.write_remote_file(rd, "slurm.sh", slu)
        log_steps.append(m2)
        if dataset_type == "graph":
            raw = base64.b64decode(pkl_b64)
            ok3, m3 = ssh.write_remote_binary(rd, "train_data.pkl", raw)
        else:
            ok3, m3 = ssh.write_remote_file(rd, "train_data.csv", dat)
        log_steps.append(m3)
        ok4, m4 = ssh.write_remote_file(rd, "config.json", cfg_content)
        log_steps.append(m4)

        if ok1 and ok2 and ok3 and ok4:
            ok_s, jid = ssh.submit_job_slurm(rd)
            if ok_s:
                return no_update, no_update, no_update, no_update, no_update, False, {"id": jid, "dir": rd, "status": "sub", "dataset_type": dataset_type, "server": active_server}, "\n".join(log_steps)+f"\næˆåŠŸ Job: {jid}", no_update
            return no_update, no_update, no_update, no_update, no_update, no_update, no_update, "\n".join(log_steps)+f"\næäº¤å¤±è´¥: {jid}", no_update

        return no_update, no_update, no_update, no_update, no_update, no_update, no_update, "\n".join(log_steps)+"\nä¸Šä¼ ä¸­æ–­", no_update

    elif trig in ["interval-job-monitor", "btn-pull-status"]:
        if not isinstance(job, dict) or not job.get("status"):
            return no_update, no_update, no_update, no_update, no_update, no_update, no_update, no_update, no_update

        if job.get("status") == "sub":
            if job.get("backend_pk"):
                refreshed, err = refresh_job_via_backend(job.get("backend_pk"))
                if err or not refreshed:
                    return no_update, no_update, no_update, no_update, no_update, no_update, no_update, f"åˆ·æ–°å¤±è´¥: {err}", no_update
                job_status = refreshed.get("status")
                job['status'] = "done" if job_status == "COMPLETED" else job_status.lower()
            else:
                job_server = job.get("server") or active_server
                ssh = RealSSHManager(**job_server)
                if ssh.connect()[0]:
                    if ssh.check_job_status(job['id']) == "COMPLETED":
                        job['status'] = "done"
                else:
                    return no_update, no_update, no_update, no_update, no_update, no_update, job, "è¿æ¥å¤±è´¥ï¼Œç¨åå†è¯•", no_update

            if job.get("status") in ["done", "COMPLETED"]:
                job_server = job.get("server") or active_server
                ssh = RealSSHManager(**job_server)
                if ssh.connect()[0]:
                    res = ssh.download_file(job['dir'], "results.csv")
                    if not res:
                        res = ssh.download_file(f"{job['dir']}/outputs/machine_learning", "results.csv")
                    ssh.close()
                    if res:
                        df = pd.read_csv(io.StringIO(res))
                        fig = go.Figure(data=go.Scatter(x=df['y_true'], y=df['y_pred'], mode='markers', text=df.get('id')))
                        fig.add_shape(type="line", x0=df['y_true'].min(), y0=df['y_true'].min(), x1=df['y_true'].max(), y1=df['y_true'].max(), line=dict(dash="dash", color="green"))
                        return no_update, no_update, no_update, no_update, no_update, no_update, job, "å®Œæˆ!", html.Div([dcc.Graph(figure=fig), dbc.Table.from_dataframe(df.head(), striped=True)])

    return no_update, no_update, no_update, no_update, no_update, no_update, no_update, no_update, no_update

if __name__ == "__main__":
    app.run_server(debug=True, port=8052)